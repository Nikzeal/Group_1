<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="author" content="Xuanlin Chen">
    <title>AI begins to be widely known</title>
    <link rel="stylesheet" href="../../templates/reset.css"> <!-- reset css -->
    <link rel="stylesheet" href="../../templates/Clock/style.css"> <!-- clock css -->
    <link rel="stylesheet" href="../../styles/2013-2023/13-23.css"> <!-- main css -->
    <!--broken links script-->
    <script src="../../scripts/det_brkn_links.js"></script>
</head>
<body>
    <!-- title -->
    <div id="title">
        <h1 class="mainTitle">AI begins to be widely known</h1>
    </div>

    <!-- content -->
    <div class="container">
        <!-- button to open hamburger menu -->
        <span id="hamburger" onclick="openNav()">&#9776;</span>

        <!-- mobile sidebar -->
        <div id="mobile_sidebar" class="overlay">
            <a href="javascript:void(0)" class="closebtn" onclick="closeNav()">&times;</a>
            <div class="overlay-content">
                <div class="side-hero">
                    <ul>
                        <li class="home"><a href="../main/index.html">Home</a></li>
                        <li class="about"><a href="../main/team.html">Team</a></li>
                        <li class="source"><a href="../main/sources.html">Sources</a></li>
                        <li class="contact"><a href="../main/link.html">Link</a></li>
                    </ul>
                    <div class="timeline-sidebar">
                        <div class="circles">
                            <!-- <div class="circle">
                                <a href="../2013-2023/2013-2015_hardware.html">2012</a>
                            </div> -->
                            <div class="circle">
                                <a href="../2013-2023/2013-2015_hardware.html">2013</a>
                            </div>
                            <div class="circle">
                                <a href="../2013-2023/2013-2015_software.html">2014</a>
                            </div>
                            <div class="circle">
                                <a href="../2013-2023/2013-2015_programming_languages.html">2015</a>
                            </div>
                            <div class="circle">
                                <a href="../2013-2023/2013-2015_historical_events.html">2015</a>
                            </div>
                            <div class="circle">
                                <a href="../2013-2023/VR-technology-booming.html">2016</a>
                            </div>
                            <div class="circle">
                                <a href="../2013-2023/Famous-Softwares.html">2016</a>
                            </div>
                            <div class="circle">
                                <a href="../2013-2023/AI-begins-to-be-widely-known.html">2017</a>
                            </div>
                            <div class="circle">
                                <a href="../2013-2023/Big-evolution-of-processors.html">2018</a>
                            </div>
                            <div class="circle">
                                <a href="../2013-2023/2013-2023-breakthrough.html">2019</a>
                            </div>
                            <div class="circle">
                                <a href="../2013-2023/2013-2023-easy-gaming.html">2019</a>
                            </div>
                            <div class="circle">
                                <a href="../2013-2023/2013-2023-samsung-and-apple.html">2020</a>
                            </div>
                            <div class="circle">
                                <a href="../2013-2023/Valve-and-Tesla.html">2021</a>
                            </div>
                            <div class="circle">
                                <a href="../2013-2023/ChatGPT-and-AIs.html">2022</a>
                            </div>
                            <div class="circle">
                                <a href="../2013-2023/Crypto-winter.html">2020</a>
                            </div>
                            <div class="circle">
                                <a href="../2013-2023/2013-2023-software-changes.html">2020</a>
                            </div>
                            <div class="circle">
                                <a href="../2013-2023/Heroes.html">2023</a>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <!-- main sidebar -->
        <div class="sidebar">
            <div class="side-hero">
                <div class="clock-container">
                    <div class="clock-hero">
                        <div class="clock">
                            <div class="hand"></div>
                        </div>
                    </div>
                    <div class="menu-hero">
                        <ul>
                            <li class="home"><a href="../main/index.html">Home</a></li>
                            <li class="about"><a href="../main/team.html">Team</a></li>
                            <li class="source"><a href="../main/sources.html">Sources</a></li>
                            <li class="contact"><a href="../main/link.html">Link</a></li>
                        </ul>
                    </div> 
                </div> 
                <div class="timeline-sidebar">
                    <div class="circles">
                        <!-- <div class="circle">
                            <a href="../2013-2023/2013-2015_hardware.html">2012</a>
                        </div> -->
                        <div class="circle">
                            <a href="../2013-2023/2013-2015_hardware.html">2013</a>
                        </div>
                        <div class="circle">
                            <a href="../2013-2023/2013-2015_software.html">2014</a>
                        </div>
                        <div class="circle">
                            <a href="../2013-2023/2013-2015_programming_languages.html">2015</a>
                        </div>
                        <div class="circle">
                            <a href="../2013-2023/2013-2015_historical_events.html">2015</a>
                        </div>
                        <div class="circle">
                            <a href="../2013-2023/VR-technology-booming.html">2016</a>
                        </div>
                        <div class="circle">
                            <a href="../2013-2023/Famous-Softwares.html">2016</a>
                        </div>
                        <div class="circle">
                            <a href="../2013-2023/AI-begins-to-be-widely-known.html">2017</a>
                        </div>
                        <div class="circle">
                            <a href="../2013-2023/Big-evolution-of-processors.html">2018</a>
                        </div>
                        <div class="circle">
                            <a href="../2013-2023/2013-2023-breakthrough.html">2019</a>
                        </div>
                        <div class="circle">
                            <a href="../2013-2023/2013-2023-easy-gaming.html">2019</a>
                        </div>
                        <div class="circle">
                            <a href="../2013-2023/2013-2023-samsung-and-apple.html">2020</a>
                        </div>
                        <div class="circle">
                            <a href="../2013-2023/Valve-and-Tesla.html">2021</a>
                        </div>
                        <div class="circle">
                            <a href="../2013-2023/ChatGPT-and-AIs.html">2022</a>
                        </div>
                        <div class="circle">
                            <a href="../2013-2023/Crypto-winter.html">2020</a>
                        </div>
                        <div class="circle">
                            <a href="../2013-2023/2013-2023-software-changes.html">2020</a>
                        </div>
                        <div class="circle">
                            <a href="../2013-2023/Heroes.html">2023</a>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <!-- main content -->
        <div class="main">
            <h2 id="AlphaGo" class="sub_title"><span class="dash">/</span>AlphaGo<span class="dash">/</span></h2>
            <br>
            <br>
            <div class="start_text">
                <div class=" text_container_gray ">
                    <p class="text">
                        DeepMind's AlphaGo defeated top Go player Lee Sedol in Seoul, South Korea, drawing comparisons to the Kasparov chess match with Deep Blue nearly 20 years earlier.
                    </p>
                    <div style="text-align: center">
                        <img class="image1" src="../../resources/2013-2023/2016-2018-14.png" alt="2016-2018-14">
                    </div>
                </div>

                <br>
                <br>
                <br>
                <br>

                <div class=" text_container_gray ">
                    <p class="text">
                        AlphaGo is a computer program that plays the board game Go. It was developed by the London-based DeepMind Technologies, an acquired subsidiary of Google (now Alphabet Inc.).
                        <br>
                        <br>
                        AlphaGo and its successors use a Monte Carlo tree search algorithm to find its moves based on knowledge previously acquired by machine learning, specifically by an artificial neural network (a deep learning method) by extensive training, both from human and computer play. A neural network is trained to identify the best moves and the winning percentages of these moves. This neural network improves the strength of the tree search, resulting in stronger move selection in the next iteration.
                        <br>
                        <br>
                        In October 2015, in a match against Fan Hui, the original AlphaGo became the first computer Go program to beat a human professional Go player without handicap on a full-sized 19×19 board. In March 2016, it beat Lee Sedol in a five-game match, the first time a computer Go program has beaten a 9-dan professional without handicap. Although it lost to Lee Sedol in the fourth game, Lee resigned in the final game, giving a final score of 4 games to 1 in favour of AlphaGo. In recognition of the victory, AlphaGo was awarded an honorary 9-dan by the Korea Baduk Association.[8] The lead up and the challenge match with Lee Sedol were documented in a documentary film also titled AlphaGo, directed by Greg Kohs. The win by AlphaGo was chosen by Science as one of the Breakthrough of the Year runners-up on 22 December 2016.
                        <br>
                        <br>
                        At the 2017 Future of Go Summit, the Master version of AlphaGo beat Ke Jie, the number one ranked player in the world at the time, in a three-game match, after which AlphaGo was awarded professional 9-dan by the Chinese Weiqi Association.
                        <br>
                        <br>
                        After the match between AlphaGo and Ke Jie, DeepMind retired AlphaGo, while continuing AI research in other areas. The self-taught AlphaGo Zero achieved a 100–0 victory against the early competitive version of AlphaGo, and its successor AlphaZero was perceived as the world's top player in Go by the end of the 2010s.
                    </p>
                </div>

                <br>
                <br>

                <div style="text-align: center">
                    <table>
                        <tr>
                            <td><img class="image" src="../../resources/2013-2023/2016-2018-15.webp" alt="2016-2018-15"></td>
                            <td><img class="image" src="../../resources/2013-2023/2016-2018-16.jpeg" alt="2016-2018-16"></td>
                        </tr>
                    </table>
                    <p>AlphaGo beat Lee Sedol in 2016 and Ke Jie in 2017 </p>
                </div>

                <br>
                <br>
                <br>
                <br>

                <h2 id="OpenAI" class="sub_title"><span class="dash">/</span>OpenAI<span class="dash">/</span></h2>
                <br>
                <br>
                <h3 class="subsub_title">OpenAI Five</h3>
                <div class=" text_container_gray ">
                    <p class="text">
                        OpenAI Five is a computer program by OpenAI that plays the five-on-five video game Dota 2. Its first public appearance occurred in 2017, where it was demonstrated in a live one-on-one game against the professional player, Dendi, who lost to it. The following year, the system had advanced to the point of performing as a full team of five, and began playing against and showing the capability to defeat professional teams.
                        <br>
                        <br>
                        By choosing a game as complex as Dota 2 to study machine learning, OpenAI thought they could more accurately capture the unpredictability and continuity seen in the real world, thus constructing more general problem-solving systems. The algorithms and code used by OpenAI Five were eventually borrowed by another neural network in development by the company, one which controlled a physical robotic hand. OpenAI Five has been compared to other similar cases of artificial intelligence (AI) playing against and defeating humans, such as AlphaStar in the video game StarCraft II, AlphaGo in the board game Go, Deep Blue in chess, and Watson on the television game show Jeopardy!.
                        <br>
                        <br>
                        <div style="text-align: center">
                            <img class="image1" src="../../resources/2013-2023/2016-2018-17.webp" height="300" width="3500" alt="2016-2018-17">
                        </div>
                </div>

                <br>
                <br>

                <div style="text-align: center">
                    <p>Comparison chart</p>
                    <table>
                        <tr>
                            <th></th>
                            <th>OpenAI 1v1 bot (2017)</th>
                            <th>OpenAI Five (2018)</th>
                        </tr>
                        <tr>
                            <td>Size of observation</td>
                            <td>~3.3kB</td>
                            <td>~36.8kB</td>
                        </tr>
                        <tr>
                            <td>Batches per minute</td>
                            <td>~20</td>
                            <td>~60</td>
                        </tr>
                        <tr>
                            <td>Experience collected</td>
                            <td>~300 years per day</td>
                            <td>~180 years per day</td>
                        </tr>
                        <tr>
                            <td>Batch size</td>
                            <td>8,388,608 observations</td>
                            <td>1,048,576 observations</td>
                        </tr>
                        <tr>
                            <td>Observations per second of gameplay</td>
                            <td>10</td>
                            <td>7.5 (12)</td>
                        </tr>
                        <tr>
                            <td>CPUs</td>
                            <td>60,000 CPU cores on Microsoft Azure</td>
                            <td>128,000 pre-emptible CPU cores on the Google Cloud Platform (GCP)</td>
                        </tr>
                        <tr>
                            <td>GPUs</td>
                            <td>256 K80 GPUs on Azure</td>
                            <td>256 P100 GPUs on the GCP</td>
                        </tr>
                    </table>
                    <br>
                    <br>
                </div>

                <h3 class="subsub_title">GPT</h3>
                <div class=" text_container_gray ">
                    <p class="text">
                        The first GPT was introduced in 2018 by OpenAI.
                        <br>
                        <br>
                        Generative pretraining (GP) was a long-established concept in machine learning applications, but the transformer architecture was not available until 2017 when it was invented by employees at Google. That development led to the emergence of large language models such as BERT in 2018 which was a pre-trained transformer (PT) but not designed to be generative (BERT was an "encoder-only" model). Also around that time, in 2018, OpenAI published its article entitled "Improving Language Understanding by Generative Pre-Training," in which it introduced the first generative pre-trained transformer (GPT) system ("GPT-1").
                        <br>
                        <br>
                        Prior to transformer-based architectures, the best-performing neural NLP (natural language processing) models commonly employed supervised learning from large amounts of manually-labeled data. The reliance on supervised learning limited their use on datasets that were not well-annotated, and also made it prohibitively expensive and time-consuming to train extremely large language models.
                        <br>
                        <br>
                        The semi-supervised approach OpenAI employed to make a large-scale generative system—and was first to do with a transformer model—involved two stages: an unsupervised generative "pretraining" stage to set initial parameters using a language modeling objective, and a supervised discriminative "fine-tuning" stage to adapt these parameters to a target task.
                    </p>
                </div>

                <br>
                <br>

                <div style="text-align: center">
                    <p>Original GPT model</p>
                    <img class="image1" src="../../resources/2013-2023/2016-2018-18.png" height="1200" width="2000" alt="2016-2018-18">
                </div>

                <br>
                <br>
                <br>
                <br>
                <div class="nofloat"></div>

                <h2 id="Google" class="sub_title"><span class="dash">/</span>Google<span class="dash">/</span></h2>
                <br>
                <br>
                <h3 style="text-align: center" class="subsub_title">Transformer</h3>
                <div class=" text_container_gray ">
                    <p class="text">
                        A transformer is a deep learning architecture, initially proposed in 2017, that relies on the parallel multi-head attention mechanism. It is notable for requiring less training time than previous recurrent neural architectures, such as long short-term memory (LSTM), and its later variation has been prevalently adopted for training large language models on large (language) datasets
                        <br>
                        <br>
                        This architecture is now used not only in natural language processing and computer vision, but also in audio[10] and multi-modal processing. It has also led to the development of pre-trained systems, such as generative pre-trained transformers (GPTs) and BERT (Bidirectional Encoder Representations from Transformers).
                    </p>
                </div>

                <br>
                <br>
                <br>
                <br>

                <div class=" text_container_gray ">
                    <p class="text">
                        In 2016, Google Translate gradually replaced the older statistical machine translation approach with the newer neural-networks-based approach that included a seq2seq model combined by LSTM and the "additive" kind of attention mechanism. They achieved a higher level of performance than the statistical approach, which took ten years to develop, in only nine months.
                        <br>
                        <br>
                        In 2017, the original (100M-sized) encoder-decoder transformer model with a faster (parallelizable or decomposable) attention mechanism was proposed in the "Attention is all you need" paper. As the model had difficulties converging, it was suggested that the learning rate should be linearly scaled up from 0 to maximal value for the first part of the training (i.e. 2% of the total number of training steps). The intent of the transformer model is to take a seq2seq model and remove its recurrent neural networks, but preserve its additive attention mechanism.
                        <br>
                        <br>
                        In 2018, in the ELMo paper, an entire sentence was processed before an embedding vector was assigning to each word in the sentence. A bi-directional LSTM was used to calculate such, deep contextualized embeddings for each word, improving upon the line of research from bag of words and word2vec,an encoder-only transformer was used in the (more than 1B-sized) BERT model, improving upon ELMo.
                    </p>
                </div>

                <br>
                <br>
                <br>
                <br>

                <p class="text">
                    <a href="#" id="gotop-btn" style="color: blue; text-decoration: underline;">Back to Top</a><!-- 直接使用锚链接来返回顶部 -->
                </p>
                <div class="text_container">
                </div>
            </div>
        </div>
    </div>

    <!-- footer -->
    <footer>
        <address>Xuanlin Chen</address>
        <p class="footer_text">
            My pages:
            <a href="../../html/2013-2023/Big-evolution-of-processors.html" style="color:white;">Big evolution of processors</a> /
            <a href="../../html/2013-2023/VR-technology-booming.html" style="color:white;">VR technology booming</a> /
            <a href="../../html/2013-2023/AI-begins-to-be-widely-known.html" style="color:white;">AI begins to be widely known</a> /
            <a href="../../html/2013-2023/Famous-Softwares.html" style="color:white;">Famous Softwares</a>
        </p>
    </footer> 

    <!-- script for mobile sidebar -->
    <script>
        function openNav() {
            document.getElementById("mobile_sidebar").style.width = "100%";
        }
        
        function closeNav() {
            document.getElementById("mobile_sidebar").style.width = "0%";
        }
    </script>

    <!-- clock script -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.1/jquery.min.js"></script>
    <script src="../../templates/Clock/script.js"></script> 
</body>
</html>